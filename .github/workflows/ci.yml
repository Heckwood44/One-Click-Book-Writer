name: One Click Book Writer CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run smoke test
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        python tests/smoke_test.py
        
    - name: Run audit
      run: |
        python scripts/run_audit.py
        
    - name: Generate CI Summary Report
      run: |
        echo "Generating CI Summary Report..."
        python -c "
        import json
        import os
        
        # Audit Report laden
        audit_file = 'output/audit_report.json'
        if os.path.exists(audit_file):
            with open(audit_file, 'r') as f:
                audit_data = json.load(f)
            
            # Meta-Datei laden
            meta_file = 'output/chapter_1_meta.json'
            meta_data = {}
            if os.path.exists(meta_file):
                with open(meta_file, 'r') as f:
                    meta_data = json.load(f)
            
            # Summary erstellen
            summary = {
                'timestamp': audit_data.get('timestamp', ''),
                'overall_success_rate': audit_data.get('summary', {}).get('success_rate', 0),
                'prompt_hash': meta_data.get('prompt_versioning', {}).get('latest_version_hash', 'N/A'),
                'system_note_detection': 'present_with_signature' if 'WORLDCLASS_AUTHOR_ARCHITECT_INVISIBLE_TRANSLATOR' in str(meta_data) else 'missing',
                'bilingual_split_ok': meta_data.get('quality_evaluation', {}).get('consistency_score', 0) > 0.5,
                'quality_score': meta_data.get('quality_evaluation', {}).get('overall_bilingual_score', 0),
                'canvas_compliance': meta_data.get('canvas_compliance', {}).get('overall_compliance', 'unknown'),
                'review_required': meta_data.get('review_required', False),
                'fallbacks': [k for k, v in meta_data.items() if 'fallback' in str(v).lower()],
                'next_recommendations': []
            }
            
            # Empfehlungen basierend auf Ergebnissen
            if summary['quality_score'] < 0.7:
                summary['next_recommendations'].append('Prompt-Optimierung für bessere Qualität')
            if summary['canvas_compliance'] != 'full':
                summary['next_recommendations'].append('Canvas-Compliance verbessern')
            if summary['review_required']:
                summary['next_recommendations'].append('Manuelle Review durchführen')
            
            # Summary speichern
            with open('output/ci_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            # Markdown Report erstellen
            with open('output/ci_summary.md', 'w') as f:
                f.write('# CI Pipeline Summary Report\n\n')
                f.write(f'**Timestamp:** {summary[\"timestamp\"]}\n')
                f.write(f'**Overall Success Rate:** {summary[\"overall_success_rate\"]}%\n')
                f.write(f'**Prompt Hash:** {summary[\"prompt_hash\"]}\n')
                f.write(f'**System Note Detection:** {summary[\"system_note_detection\"]}\n')
                f.write(f'**Bilingual Split OK:** {summary[\"bilingual_split_ok\"]}\n')
                f.write(f'**Quality Score:** {summary[\"quality_score\"]}\n')
                f.write(f'**Canvas Compliance:** {summary[\"canvas_compliance\"]}\n')
                f.write(f'**Review Required:** {summary[\"review_required\"]}\n')
                f.write(f'**Fallbacks Applied:** {len(summary[\"fallbacks\"])}\n')
                f.write(f'**Next Recommendations:** {len(summary[\"next_recommendations\"])}\n\n')
                
                if summary['next_recommendations']:
                    f.write('## Recommendations\n')
                    for rec in summary['next_recommendations']:
                        f.write(f'- {rec}\n')
        
        print('CI Summary Report generated successfully')
        "
        
    - name: Upload test artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-outputs
        path: |
          output/chapter_*.txt
          output/chapter_*.json
          output/audit_report.*
          output/ci_summary.*
          output/chapter_*_prompt_diff.json
        retention-days: 7
        
    - name: Check quality threshold
      run: |
        if [ -f "output/audit_report.json" ]; then
          python -c "import json; data=json.load(open('output/audit_report.json')); success_rate=data['summary']['success_rate']; print(f'Quality: {success_rate:.1f}%'); exit(0 if success_rate >= 70 else 1)"
        fi 